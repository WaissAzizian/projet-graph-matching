{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quadratic assignment problem using GNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic assignment problem\n",
    "[Nowak, A., Villar, S., Bandeira, A. S., & Bruna, J. (2017). A note on learning algorithms for quadratic assignment with graph neural networks.](https://arxiv.org/abs/1706.07450)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Graph isomorphism:\n",
    " \n",
    " $G_1 = (V_1, E_1)$ and $G_2 = (V_2, E_2)$ are isomorphic if there is a bijection $V_1 \\longrightarrow V_2$ which preserves edges.\n",
    " \n",
    " \n",
    " - QAP: \"noisy isomorphism problem\"\n",
    " \n",
    " If $A_1, A_2$ adjacency matrices,\n",
    " $$\\min \\|P^{-1}A_1P - A_2\\|\\quad \\text{ s.t. } P \\text{ permutation matrix}$$\n",
    " \n",
    " \n",
    " - Data generation for training\n",
    " \n",
    " Pick $G_1$ according to some distribution on graphs, and $$G_2 = G_1 + w$$\n",
    " where $w$ is some noise.\n",
    " \n",
    " Fox example: if $G$ follows an Erdos-Renyi distribution, then $$\n",
    " E_{ij}=\\begin{cases}\n",
    " 1 &\\text{ with probability } p\\\\\n",
    " 0 &\\text{ with probability } 1 - p\n",
    " \\end{cases}\n",
    " $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  [Xu, K., Hu, W., Leskovec, J., & Jegelka, S. (2018). How Powerful are Graph Neural Networks?](https://arxiv.org/abs/1810.00826)\n",
    "  \n",
    "  [Maron, H., Ben-Hamu, H., Shamir, N., & Lipman, Y. (2018). Invariant and equivariant graph networks.](https://arxiv.org/abs/1812.09902)\n",
    "  \n",
    "  [Maron, H., Ben-Hamu, H., Serviansky, H., & Lipman, Y. (2019). Provably Powerful Graph Networks.](https://arxiv.org/abs/1905.11136)\n",
    "- Message-passing GNN:\n",
    "\n",
    "    $$h_v^t = \\text{COMBINE}\\left(h_v^{t-1}, \\text{AGGREGATE}\\left(h_u^{t-1}: u \\sim v\\right)\\right)$$\n",
    "  \n",
    "  \n",
    " - $k$-th order networks: succession of equivariant linear layers and non linearities\n",
    "\n",
    "An equivariant linear layer: $L: \\mathbb{R}^{n^k \\times a} \\longrightarrow \\mathbb{R}^{n^k \\times a}$ s.t.\n",
    "$$L(\\sigma \\cdot X) = \\sigma \\cdot L(X)$$\n",
    "where $\\sigma \\in S_n$ permutation, $(\\sigma \\cdot X)_{i_1,\\dots,i_k,j} = X_{\\sigma^{-1}(i_1),\\dots,\\sigma^{-1}(i_k),j}$ and $n$ number of vertices.\n",
    " \n",
    "\n",
    " \n",
    " \n",
    " - Our architecture: $2$-nd order network, striclty more powerful than message passing networks\n",
    " \n",
    " Initially $\\mathbb R^{n^2 \\times a} \\xrightarrow{B_1} \\mathbb R^{n^2 \\times b} \\xrightarrow{B_2} \\mathbb R^{n^2 \\times b} \\xrightarrow{B_3} \\mathbb R^{n^2 \\times b} \\xrightarrow{B'} \\mathbb R^{n \\times c}$\n",
    " \n",
    " $B_1, B_2, B_3$ are blocks combining MLPs and matrix multiplication. $B'$ is either averaging over rows or a more sophisticated equivariant layer.\n",
    " \n",
    "\n",
    " \n",
    " \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and next steps\n",
    "For now, with both choices of $B'$, accuracy stays at about $15\\%$...\n",
    "### Next steps\n",
    "- Tune parameters and architecture\n",
    "- Try with other architectures\n",
    "- Improve the generation of the assignment\n",
    "- Use for pretraining ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
